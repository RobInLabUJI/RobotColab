{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Detection\n",
    "\n",
    "<img align=\"right\" src=\"img/road_segmentation.jpg\" />\n",
    "The result of image processing was a binary image, or mask, with the pixels that belong to the line, i.e. the cyan-colored pixels.\n",
    "\n",
    "For driving the robot, we need to compute some value relating the position of the robot to the line in the ground. In this task, it is sufficient to keep the line centered in the image. For more complex tasks, there are algorithms that compute the geometrical parameters of the line image, and its 3D reconstruction in real space.\n",
    "\n",
    "Our method is far simpler: we consider the line as a blob in the image, whose [*image moments*](http://aishack.in/tutorials/image-moments/) can be computed, particularly its [*centroid*](https://en.wikipedia.org/wiki/Image_moment#Central_moments). That information will be used for later driving the robot appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: before running the code below, please restart the simulation in Webots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pioneer3.Controllers import PioneerRobot\n",
    "\n",
    "robot = PioneerRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = robot.kinect.getColorImage()\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "lower_cyan = numpy.array([80, 100, 100])\n",
    "upper_cyan = numpy.array([100, 255, 255])\n",
    "mask = cv2.inRange(hsv, lower_cyan, upper_cyan)\n",
    "plt.imshow(mask,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the Centroid\n",
    "\n",
    "We use here some heuristics: first, the Kinect should tilt down for observing the line close to the robot, not far away; second, we will only consider the bottom part of the line for computing the image moments; doing so will prevent the robot to turn before it actually arrives to the curve. In practice, we will set all the pixels to black (zeros) for the lines between 0 and 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[0:80, 0:150] = 0\n",
    "plt.imshow(mask,cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the moments, the [centroid](https://en.wikipedia.org/wiki/Image_moment#Central_moments), and display the original image, with a red circle at the position of the centroid of the computed blob. If the result is correct, the circle should be centered on the bottom of the cyan line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = cv2.moments(mask)\n",
    "cx = int(M['m10']/M['m00'])\n",
    "cy = int(M['m01']/M['m00'])\n",
    "plt.imshow(image)\n",
    "axes = plt.gca()\n",
    "axes.add_artist(plt.Circle((cx,cy),10,color='r'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: [Line Following](Line%20Following.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Try-a-Bot: an open source guide for robot programming\n",
    "Developed by:\n",
    "[![Robotic Intelligence Lab @ UJI](img/logo/robinlab.png \"Robotic Intelligence Lab @ UJI\")](http://robinlab.uji.es)\n",
    "\n",
    "Sponsored by:\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"border:1px solid #ffffff ;\">\n",
    "<a href=\"http://www.ieee-ras.org\"><img src=\"img/logo/ras.png\"></a>\n",
    "</td>\n",
    "<td style=\"border:1px solid #ffffff ;\">\n",
    "<a href=\"http://www.cyberbotics.com\"><img src=\"img/logo/cyberbotics.png\"></a>\n",
    "</td>\n",
    "<td style=\"border:1px solid #ffffff ;\">\n",
    "<a href=\"http://www.theconstructsim.com\"><img src=\"img/logo/theconstruct.png\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Follow us:\n",
    "<table>\n",
    "<tr>\n",
    "<td style=\"border:1px solid #ffffff ;\">\n",
    "<a href=\"https://www.facebook.com/RobotProgrammingNetwork\"><img src=\"img/logo/facebook.png\"></a>\n",
    "</td>\n",
    "<td style=\"border:1px solid #ffffff ;\">\n",
    "<a href=\"https://www.youtube.com/user/robotprogrammingnet\"><img src=\"img/logo/youtube.png\"></a>\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
